DATA:
  DATASET: inat21
  IMG_SIZE: 192
  DATA_PATH: /mnt/10tb/data/inat21/resize-192
  NUM_WORKERS: 32
  BATCH_SIZE: 64
MODEL:
  TYPE: swinv2
  NAME: sweet-strawberry-192
  DROP_PATH_RATE: 0.2
  SWINV2:
    EMBED_DIM: 128
    DEPTHS: [ 2, 2, 18, 2 ]
    NUM_HEADS: [ 4, 8, 16, 32 ]
    WINDOW_SIZE: 12
TRAIN:
  # Want a global batch size of 2048 because SwinV2 was trained on 16 V100s with batch size 128 (I think)
  # But we are going to use a global batch size of 1024 because it's faster (throughput).
  ACCUMULATION_STEPS: 2

  # We are using limited epochs based on pre-training configs for imagenet22k
  # Then we will pre-train on 256x256 for 30 epochs
  EPOCHS: 90
  WARMUP_EPOCHS: 5
  WEIGHT_DECAY: 0.1

  # Use 1/2 of original learning rates
  BASE_LR: 2.5e-4
  WARMUP_LR: 2.5e-7
  MIN_LR: 2.5e-6
  HIERARCHICAL_COEFFS: [ 8, 5.65, 4, 2.82, 2, 1.41, 1 ]

HIERARCHICAL: true
